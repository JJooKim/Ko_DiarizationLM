{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGBxW7ydPE8-"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oOszGeoI--m",
        "outputId": "06d62405-3285-4894-8ca4-38f6ae56513c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7bqFFOreKVJ",
        "outputId": "12d10170-19e9-4035-dedb-d6a38afa3272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Projects/ko_diarizationLM\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Projects/ko_diarizationLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVmnfthELB3r",
        "outputId": "50cda578-fce5-4650-ad34-e4ea69a764c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Projects/ko_diarizationLM/Data/Training\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Projects/ko_diarizationLM/Data/Training\n",
        "\n",
        "!unzip -u -qq \"/content/drive/MyDrive/Projects/ko_diarizationLM/Data/Training/[라벨]KtelSpeech_train_D60_label_1.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzo6cEinLX0D",
        "outputId": "7bcff90a-1c29-4346-d41f-4fa3574b8d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mJ91\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SXrfNF3vzaf"
      },
      "source": [
        "### Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn1c-CoDv2kw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/m-bain/whisperX.git@a5dca2cc65b1a37f32a347e574b2c56af3a7434a\n",
        "!pip install --no-build-isolation nemo_toolkit[asr]==1.21.0\n",
        "!pip install git+https://github.com/facebookresearch/demucs#egg=demucs\n",
        "!pip install deepmultilingualpunctuation\n",
        "!pip install wget pydub\n",
        "# !pip install --force-reinstall torch torchaudio torchvision\n",
        "# !pip uninstall -y nvidia-cudnn-cu12\n",
        "!pip install numba==0.58.0\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8dccH7vsyv"
      },
      "source": [
        "**RESTART the runtime now!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvJ3VVUOv14T"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzhncHP0ytbQ",
        "outputId": "fb5610ea-3cbb-48a3-b142-af1095818beb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "[NeMo W 2024-02-11 14:20:23 transformer_bpe_models:59] Could not import NeMo NLP collection which is required for speech translation model.\n"
          ]
        }
      ],
      "source": [
        "v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsUt3SwyhjD"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se6Hc7CZygxu"
      },
      "outputs": [],
      "source": [
        "punct_model_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"bg\",\n",
        "    \"pl\",\n",
        "    \"cs\",\n",
        "    \"sk\",\n",
        "    \"sl\",\n",
        "]\n",
        "wav2vec2_langs = list(DEFAULT_ALIGN_MODELS_TORCH.keys()) + list(\n",
        "    DEFAULT_ALIGN_MODELS_HF.keys()\n",
        ")\n",
        "\n",
        "whisper_langs = sorted(LANGUAGES.keys()) + sorted(\n",
        "    [k.title() for k in TO_LANGUAGE_CODE.keys()]\n",
        ")\n",
        "\n",
        "\n",
        "def create_config(output_dir, DOMAIN_TYPE = \"telephonic\"):\n",
        "    # DOMAIN_TYPE: can be meeting, telephonic, or general based on domain type of the audio file\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)\n",
        "\n",
        "    config = OmegaConf.load(MODEL_CONFIG)\n",
        "\n",
        "    data_dir = os.path.join(output_dir, \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    meta = {\n",
        "        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n",
        "        \"offset\": 0,\n",
        "        \"duration\": None,\n",
        "        \"label\": \"infer\",\n",
        "        \"text\": \"-\",\n",
        "        \"rttm_filepath\": None,\n",
        "        \"uem_filepath\": None,\n",
        "    }\n",
        "    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n",
        "        json.dump(meta, fp)\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
        "    pretrained_speaker_model = \"titanet_large\"\n",
        "    config.num_workers = 0  # Workaround for multiprocessing hanging with ipython issue\n",
        "    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n",
        "    config.diarizer.out_dir = (\n",
        "        output_dir  # Directory to store intermediate files and prediction outputs\n",
        "    )\n",
        "\n",
        "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
        "    config.diarizer.oracle_vad = (\n",
        "        False  # compute VAD provided with model_path to vad config\n",
        "    )\n",
        "    config.diarizer.clustering.parameters.oracle_num_speakers = False\n",
        "\n",
        "    # Here, we use our in-house pretrained NeMo VAD model\n",
        "    config.diarizer.vad.model_path = pretrained_vad\n",
        "    config.diarizer.vad.parameters.onset = 0.8\n",
        "    config.diarizer.vad.parameters.offset = 0.6\n",
        "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
        "    config.diarizer.msdd_model.model_path = (\n",
        "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_word_ts_anchor(s, e, option=\"start\"):\n",
        "    if option == \"end\":\n",
        "        return e\n",
        "    elif option == \"mid\":\n",
        "        return (s + e) / 2\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "    for wrd_dict in wrd_ts:\n",
        "        ws, we, wrd = (\n",
        "            int(wrd_dict[\"start\"] * 1000),\n",
        "            int(wrd_dict[\"end\"] * 1000),\n",
        "            wrd_dict[\"word\"],\n",
        "        )\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "            if turn_idx == len(spk_ts) - 1:\n",
        "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
        "        wrd_spk_mapping.append(\n",
        "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
        "        )\n",
        "    return wrd_spk_mapping\n",
        "\n",
        "\n",
        "sentence_ending_punctuations = \".?!\"\n",
        "\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    left_idx = word_idx\n",
        "    while (\n",
        "        left_idx > 0\n",
        "        and word_idx - left_idx < max_words\n",
        "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
        "        and not is_word_sentence_end(left_idx - 1)\n",
        "    ):\n",
        "        left_idx -= 1\n",
        "\n",
        "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    right_idx = word_idx\n",
        "    while (\n",
        "        right_idx < len(word_list)\n",
        "        and right_idx - word_idx < max_words\n",
        "        and not is_word_sentence_end(right_idx)\n",
        "    ):\n",
        "        right_idx += 1\n",
        "\n",
        "    return (\n",
        "        right_idx\n",
        "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
        "        else -1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(\n",
        "    word_speaker_mapping, max_words_in_sentence=50\n",
        "):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0\n",
        "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "    words_list, speaker_list = [], []\n",
        "    for k, line_dict in enumerate(word_speaker_mapping):\n",
        "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
        "        words_list.append(word)\n",
        "        speaker_list.append(speaker)\n",
        "\n",
        "    k = 0\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k]\n",
        "        if (\n",
        "            k < wsp_len - 1\n",
        "            and speaker_list[k] != speaker_list[k + 1]\n",
        "            and not is_word_sentence_end(k)\n",
        "        ):\n",
        "            left_idx = get_first_word_idx_of_sentence(\n",
        "                k, words_list, speaker_list, max_words_in_sentence\n",
        "            )\n",
        "            right_idx = (\n",
        "                get_last_word_idx_of_sentence(\n",
        "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
        "                )\n",
        "                if left_idx > -1\n",
        "                else -1\n",
        "            )\n",
        "            if min(left_idx, right_idx) == -1:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
        "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
        "                right_idx - left_idx + 1\n",
        "            )\n",
        "            k = right_idx\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    k, realigned_list = 0, []\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k].copy()\n",
        "        line_dict[\"speaker\"] = speaker_list[k]\n",
        "        realigned_list.append(line_dict)\n",
        "        k += 1\n",
        "\n",
        "    return realigned_list\n",
        "\n",
        "\n",
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
        "    s, e, spk = spk_ts[0]\n",
        "    prev_spk = spk\n",
        "\n",
        "    snts = []\n",
        "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
        "\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "        if spk != prev_spk or sentence_checker(snt[\"text\"] + \" \" + wrd):\n",
        "            snts.append(snt)\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            snt[\"end_time\"] = e\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        prev_spk = spk\n",
        "\n",
        "    snts.append(snt)\n",
        "    return snts\n",
        "\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
        "    previous_speaker = sentences_speaker_mapping[0][\"speaker\"]\n",
        "    f.write(f\"{previous_speaker}: \")\n",
        "\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        speaker = sentence_dict[\"speaker\"]\n",
        "        sentence = sentence_dict[\"text\"]\n",
        "\n",
        "        # If this speaker doesn't match the previous one, start a new paragraph\n",
        "        if speaker != previous_speaker:\n",
        "            f.write(f\"\\n\\n{speaker}: \")\n",
        "            previous_speaker = speaker\n",
        "\n",
        "        # No matter what, write the current sentence\n",
        "        f.write(sentence + \" \")\n",
        "\n",
        "\n",
        "def format_timestamp(\n",
        "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
        "):\n",
        "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return (\n",
        "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
        "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def find_numeral_symbol_tokens(tokenizer):\n",
        "    numeral_symbol_tokens = [\n",
        "        -1,\n",
        "    ]\n",
        "    for token, token_id in tokenizer.get_vocab().items():\n",
        "        has_numeral_symbol = any(c in \"0123456789%$£\" for c in token)\n",
        "        if has_numeral_symbol:\n",
        "            numeral_symbol_tokens.append(token_id)\n",
        "    return numeral_symbol_tokens\n",
        "\n",
        "\n",
        "def _get_next_start_timestamp(word_timestamps, current_word_index):\n",
        "    # if current word is the last word\n",
        "    if current_word_index == len(word_timestamps) - 1:\n",
        "        return word_timestamps[current_word_index][\"start\"]\n",
        "\n",
        "    next_word_index = current_word_index + 1\n",
        "    while current_word_index < len(word_timestamps) - 1:\n",
        "        if word_timestamps[next_word_index].get(\"start\") is None:\n",
        "            # if next word doesn't have a start timestamp\n",
        "            # merge it with the current word and delete it\n",
        "            word_timestamps[current_word_index][\"word\"] += (\n",
        "                \" \" + word_timestamps[next_word_index][\"word\"]\n",
        "            )\n",
        "\n",
        "            word_timestamps[next_word_index][\"word\"] = None\n",
        "            next_word_index += 1\n",
        "\n",
        "        else:\n",
        "            return word_timestamps[next_word_index][\"start\"]\n",
        "\n",
        "\n",
        "def filter_missing_timestamps(word_timestamps):\n",
        "    # handle the first and last word\n",
        "    if word_timestamps[0].get(\"start\") is None:\n",
        "        word_timestamps[0][\"start\"] = 0\n",
        "        word_timestamps[0][\"end\"] = _get_next_start_timestamp(word_timestamps, 0)\n",
        "\n",
        "    result = [\n",
        "        word_timestamps[0],\n",
        "    ]\n",
        "\n",
        "    for i, ws in enumerate(word_timestamps[1:], start=1):\n",
        "        # if ws doesn't have a start and end\n",
        "        # use the previous end as start and next start as end\n",
        "        if ws.get(\"start\") is None and ws.get(\"word\") is not None:\n",
        "            ws[\"start\"] = word_timestamps[i - 1][\"end\"]\n",
        "            ws[\"end\"] = _get_next_start_timestamp(word_timestamps, i)\n",
        "\n",
        "        if ws[\"word\"] is not None:\n",
        "            result.append(ws)\n",
        "    return result\n",
        "\n",
        "\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))\n",
        "\n",
        "\n",
        "def process_language_arg(language: str, model_name: str):\n",
        "    \"\"\"\n",
        "    Process the language argument to make sure it's valid and convert language names to language codes.\n",
        "    \"\"\"\n",
        "    if language is not None:\n",
        "        language = language.lower()\n",
        "    if language not in LANGUAGES:\n",
        "        if language in TO_LANGUAGE_CODE:\n",
        "            language = TO_LANGUAGE_CODE[language]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported language: {language}\")\n",
        "\n",
        "    if model_name.endswith(\".en\") and language != \"en\":\n",
        "        if language is not None:\n",
        "            logging.warning(\n",
        "                f\"{model_name} is an English-only model but received '{language}'; using English instead.\"\n",
        "            )\n",
        "        language = \"en\"\n",
        "    return language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqN2ffJ-IXqO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxDrdPzKD4_s"
      },
      "outputs": [],
      "source": [
        "# no space, punctuation, accent in lower string\n",
        "def cleanString(string):\n",
        "    cleanString = unidecode(string)\n",
        "    # cleanString = re.sub('\\W+','_', cleanString)\n",
        "    cleanString = re.sub(r'[^\\w\\s]','',cleanString)\n",
        "    cleanString = cleanString.replace(\" \", \"_\")\n",
        "    return cleanString.lower()\n",
        "\n",
        "# rename audio filename to get name without accent, no space, in lower case\n",
        "def rename_file(filepath):\n",
        "    suffix = Path(filepath).suffix\n",
        "    if str(Path(filepath).parent) != \".\":\n",
        "        new_filepath = str(Path(filepath).parent) + cleanString(filepath.replace(suffix, \"\")) + suffix\n",
        "    else:\n",
        "        new_filepath = cleanString(filepath.replace(suffix, \"\")) + suffix\n",
        "    os.rename(filepath, new_filepath)\n",
        "    return new_filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_batched(\n",
        "    audio_file: str,\n",
        "    language: str,\n",
        "    batch_size: int,\n",
        "    device: str\n",
        "):\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "    result = whisper_model.transcribe(audio, language=language, batch_size=batch_size)\n",
        "    #del whisper_model\n",
        "    torch.cuda.empty_cache()\n",
        "    return result[\"segments\"], result[\"language\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7qWQb--1Xcw"
      },
      "source": [
        "### Key Fuction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_m-QuC-w8MB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import whisperx\n",
        "\n",
        "# whisper model\n",
        "whisper_model = whisperx.load_model(\n",
        "    \"large-v3\",\n",
        "    device=\"cuda\",\n",
        "    compute_type=\"float16\",\n",
        "    asr_options={\"suppress_numerals\": True},\n",
        "    language=\"ko\"\n",
        ")\n",
        "\n",
        "# allignment_model\n",
        "alignment_model, metadata = whisperx.load_align_model(\n",
        "language_code=\"ko\", device=\"cuda\"\n",
        ")\n",
        "\n",
        "# ? Check purpose\n",
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
        "os.makedirs(temp_path, exist_ok=True)\n",
        "\n",
        "def get_hyp_json(audio_path, batch_size=8, language=\"ko\", device=\"cuda\"):\n",
        "    enable_stemming = False\n",
        "\n",
        "    rename_file(audio_path)\n",
        "\n",
        "    ## 1. Separating music from speech using Demucs\n",
        "    if enable_stemming:\n",
        "        # Isolate vocals from the rest of the audio\n",
        "\n",
        "        return_code = os.system(\n",
        "            f'python3 -m demucs.separate -n htdemucs --two-stems=vocals \"{audio_path}\" -o \"temp_outputs\"'\n",
        "        )\n",
        "\n",
        "        if return_code != 0:\n",
        "            logging.warning(\"Source splitting failed, using original audio file.\")\n",
        "            vocal_target = audio_path\n",
        "        else:\n",
        "            vocal_target = os.path.join(\n",
        "                \"temp_outputs\",\n",
        "                \"htdemucs\",\n",
        "                os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                \"vocals.wav\",\n",
        "            )\n",
        "    else:\n",
        "        vocal_target = audio_path\n",
        "\n",
        "    ## 2.Transcriping audio using Whisper and realligning timestamps using Wav2Vec2\n",
        "    whisper_results, language = transcribe_batched(\n",
        "        vocal_target,\n",
        "        language,\n",
        "        batch_size,\n",
        "        device\n",
        "    )\n",
        "\n",
        "\n",
        "    ## 3.Aligning the transcription with the original audio using Wav2Vec2\n",
        "    result_aligned = whisperx.align(\n",
        "        whisper_results, alignment_model, metadata, vocal_target, device\n",
        "    )\n",
        "    word_timestamps = filter_missing_timestamps(result_aligned[\"word_segments\"])\n",
        "\n",
        "    # clear gpu vram\n",
        "    #del alignment_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 4.\n",
        "    sound = AudioSegment.from_file(vocal_target).set_channels(1)\n",
        "    output_file_path = os.path.join(temp_path, \"mono_file.wav\")\n",
        "    sound.export(output_file_path, format=\"wav\")\n",
        "    msdd_model = NeuralDiarizer(cfg=create_config(temp_path, DOMAIN_TYPE=\"telephonic\")).to(\"cuda\")\n",
        "    msdd_model.diarize()\n",
        "    os.remove(output_file_path)\n",
        "    del msdd_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    speaker_ts = []\n",
        "    with open(os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\"), \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line_list = line.split(\" \")\n",
        "            s = int(float(line_list[5]) * 1000)\n",
        "            e = s + int(float(line_list[8]) * 1000)\n",
        "            speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
        "\n",
        "    wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")\n",
        "\n",
        "    wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "    #ssm = get_sentences_speaker_mapping(wsm, speaker_ts)\n",
        "    return wsm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSwOEyIR-gBn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFOx7ikdRKeM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wave\n",
        "from moviepy.editor import concatenate_audioclips, AudioFileClip\n",
        "\n",
        "def concatenate_wav_files(directory_path, output_file):\n",
        "    wav_files = [f for f in os.listdir(directory_path) if f.endswith('.wav')]\n",
        "    if 'full.wav' in wav_files:\n",
        "        wav_files.remove('full.wav')\n",
        "    wav_files.sort()  # Sort files in ascending order\n",
        "    clips = [AudioFileClip(os.path.join(directory_path, wav_file)) for wav_file in wav_files]\n",
        "    final_clip = concatenate_audioclips(clips)\n",
        "    final_clip.write_audiofile(output_file)\n",
        "    wsm = get_hyp_json(output_file)\n",
        "    os.remove(output_file)\n",
        "    return wsm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V17Kvg0ioCS"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Projects/ko_diarizationLM/data_500.json', 'r', encoding='utf-8-sig') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "data = json_data['utterances']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwwj_taJi6O9",
        "outputId": "aabcdcb4-ca3b-4eba-af39-67d267a84930"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'utterance_id': 'Data/Training/D60/J91/S00011787',\n",
              " 'ref_text': '안녕하세요 엔 교육 상담원 김지영입니다 네 안녕하세요 어떤 일로 전화 주셨을까요 제가 엔포탈 뒤지다 보니까 엔 교육 세무사 삼 년 영 원 합격 패스 과정이 있더라고요 요새 코로나 때문에 집에만 처박혀 있어 있는데 자격증 공부를 할 수 있으면 좋겠다 싶어서요 이거 진짜 삼 년 동안 진짜 무료인 거예요 영 원 아니면 무슨 조건이 있는 것인지 궁금해서 전화드렸어요 그리고 세무사 자격증 취득할 때 필요한 모든 과목 다 가르쳐주는 건가요 아 일단은 세무사 시험 응시에 필요한 모든 과목을 다 가르쳐주는 거긴 하군요 시간이나 배수 제한 없이 자유롭게 무조건 무제한 수강은 가능하고요 엔 교육 특별 제작 교재 네 권이 무료로 제공되기도 하고요 이거 강의 무제한 수강은 컴퓨터로 듣는 것만 가능한 거예요 모바일로도 가능합니다 아 핸드폰으로도 가능해요 혹시 그럼 태블릿으로도 가능한 건가요 기기는 이이 두 개까지 가능합니다 아 그러면 핸드폰 하나 등록하고 컴퓨터 하나 등록하고 해서 두 개까지 등록이 된다는 말인 거네요 아니면 태블릿 하나 등록하고 컴퓨터 하나 등록하고 해서 두 개까지 등록이 가능하다는 거고요 그러면 태블릿을 등록하고 나서 나중에 핸드폰으로 바꿀 수도 있나요 아니면 처음 등록하면 끝이에요 만약에 중간에 핸드폰이 고장 나서 기기변경을 하게 되면 그 땐 무조건 컴퓨터로만 들어야 하는 건가요 등록변경이 가능합니다 아 그 때 되면 고객센터에 전화해서 기기 등록 다시 바꿔주면 된다고요 그래요 역시 전문적인 교육기관 엔 교육 같네요 그건 그럼 걱정이 안 돼요 그러면 그냥 삼 년 동안 무제한으로 강좌를 듣는 건가요 우선 일 년입니다 아 그러면 일단은 기본적으로 십 이 개월 그러니까 일 년 동안 강의를 수강하고 듣는 거네요 그 다음에 시험 응시해서 불합격한 내용을 인증해 주면 연장으로 십 이 개월 또 불합격하면 연장으로 십 이 개월 그렇게 해서 총 삼 년이라는 거죠 혹시 그러면 그 이후에도 못 붙으면요 그러면 환불 안됩니다 와 그럼 무조건 합격을 해야지 학원비가 무료인 거네요 동기부여는 확실히 되긴 할 텐데 능력이 안돼서 못 붙으면 학원비는 그냥 날리는 거 아닌가요 무료라고 기재하면 안 될 것 같은데요 무튼 그러면 이건 세무사 시험 일 차 준비과정인 건가요 일 이 차 다 입니다 아 강좌는 일 차 시험과목과 이 차 시험과목까지 모두 다 가르쳐주기는 한다는 거죠 혹시 그러면 나중에 객관식 문제풀이반이나 집중 모의고사반 같은 게 있나요 환급과정엔 포함이 안 됩니다 아 그러면 객관식 문제풀이반이나 집중 모의 고사반 같은 과목들은 별도로 따로 신청해서 수강해야 하는 거네요 역시 자격증 따는데 돈이 많이 들긴 하네요 삼 년 안에도 못 붙으면 그 학원비 그대로 그냥 다 날리는 것이다 보니 좀 고민이 되기는 해요 특히 저는 인강으로 들으면 집중해서 안 듣고 틀어놓고 그냥 잠들기도 하는 데다가 세무사 시험 과목들이 아무래도 어렵거나 모르는 내용이 생길 수도 있으니 질의응답이 가능한 과정이 저한테는 더 좋거든요 혹시 그러면 이런 환급반은 오프라인으로 진행되는 건가요 코로나로 아직까지 온라인으로 합니다 아 코로나 때문에 현재까지는 온라인 강의만 진행된다는 거죠 그러면 질문은 어떻게 할 수 있어요 게시판을 이용하시면 됩니다 별도로 강사님께 일 대 일 질문하기 게시판을 운영하니 거기다가 물어보라는 거죠 제 궁금증은 해결할 수 있긴 하겠네요 이런 건 바로 답변이 오는 건가요 강사님들이 바로 바로 확인을 해주는 편입니다 그래요 그래도 바로 바로 확인해서 답변을 달아주신다는 거죠 일단은 그럼 좀 더 고민해 볼게요 감사합니다',\n",
              " 'ref_spk': '1 1 1 1 1 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[1150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4jIrO3VSXI4"
      },
      "outputs": [],
      "source": [
        "  \"utterances\": [\n",
        "    {\n",
        "      \"utterance_id\": \"ko_0001\",\n",
        "      \"hyp_text\":,\n",
        "      \"ref_text\":,\n",
        "      \"ref_spk\": ,\n",
        "      \"ref_diarized_text\": \"<speaker:1> Right. <speaker:2> And I I already got another apartment for when I move out.,\n",
        "      \"hyp_spk_oracle\":\n",
        "      \"hyp_diarized_text_oracle\":\n",
        "      \"ref_spk_degraded\":\n",
        "      \"ref_diarized_text_degraded\":\n",
        "    },"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCUglgOwQ6bg"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open('/content/drive/MyDrive/Projects/ko_diarizationLM/data_500.json', 'r', encoding='utf-8-sig') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "def add_hyp_values(utterances):\n",
        "    i = 0\n",
        "    for utterance in utterances[1200:1300]:\n",
        "        print(i)\n",
        "        i += 1\n",
        "        dir_path = utterance[\"utterance_id\"]\n",
        "        output_file = 'full.wav'\n",
        "        wsm = concatenate_wav_files(dir_path, output_file)\n",
        "        # hyp_texts = [re.sub(r'[^가-힣]', '', entry['word']) for entry in wsm]\n",
        "        # hyp_spks = [str(int(entry['speaker']) + 1) for entry in wsm]\n",
        "        hyp_texts = []\n",
        "        hyp_spks = []\n",
        "        for entry in wsm:\n",
        "            txt = re.sub(r'[^가-힣]', '', entry['word'])\n",
        "            if txt == '':\n",
        "                continue\n",
        "            else:\n",
        "                hyp_texts.append(txt)\n",
        "                hyp_spks.append(str(int(entry['speaker']) + 1))\n",
        "\n",
        "        utterance['hyp_text'] = ' '.join(hyp_texts)\n",
        "        utterance['hyp_spk'] = ' '.join(hyp_spks)\n",
        "        if (i % 50) == 0:\n",
        "            with open('/content/drive/MyDrive/Projects/ko_diarizationLM/data_500.json', 'w', encoding='utf-8-sig') as file:\n",
        "                json.dump(json_data, file, indent=2)\n",
        "\n",
        "\n",
        "    with open('/content/drive/MyDrive/Projects/ko_diarizationLM/data_500.json', 'w', encoding='utf-8-sig') as file:\n",
        "        json.dump(json_data, file, indent=2)\n",
        "\n",
        "add_hyp_values(json_data[\"utterances\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUuWi_n7iQ7k"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Projects/ko_diarizationLM/data_500.json', 'w', encoding='utf-8-sig') as file:\n",
        "    json.dump(json_data, file, indent=2)#189 까지 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZOMpPyniQ48"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl3ma8tXiQ2W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmXNY5eLkMX0",
        "outputId": "3a8c1bc2-19bc-4d55-b4ad-353a6a9a5fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 2 1 1 1 2 2 2\n"
          ]
        }
      ],
      "source": [
        "def switch_ones_and_twos(input_str):\n",
        "    # Count the number of 1's and 2's\n",
        "    count_ones = input_str.count('1')\n",
        "    count_twos = input_str.count('2')\n",
        "\n",
        "    # Check if there are more 2's than 1's\n",
        "    if count_twos < count_ones:\n",
        "        # Switch 1's and 2's in the string\n",
        "        switched_str = input_str.replace('1', 'temp').replace('2', '1').replace('temp', '2')\n",
        "        return switched_str\n",
        "    else:\n",
        "        return input_str\n",
        "\n",
        "# Example usage\n",
        "input_string = \"2 2 1 1 1 2 2 2\"\n",
        "result = switch_ones_and_twos(input_string)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLViLDFI0-3v"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Projects/ko_diarizationLM/data_re.json', 'r', encoding='utf-8-sig') as file:\n",
        "    json_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GxSW0vI3Gjg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
